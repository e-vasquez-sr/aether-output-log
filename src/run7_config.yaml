# Project Aether — Training Run 7 Configuration
# Created: 2024-11-01T00:00:00Z

model:
  architecture: transformer
  variant: aether-7b-v2
  num_layers: 96
  hidden_size: 4096
  num_attention_heads: 32
  intermediate_size: 11008
  max_position_embeddings: 4096
  vocab_size: 32000
  activation_function: silu
  tie_word_embeddings: false

training:
  run_id: aether-run-7
  run_alias: subject-seven
  epochs: 10000
  batch_size: 256
  gradient_accumulation_steps: 8
  learning_rate: 3.0e-4
  lr_scheduler: cosine
  warmup_steps: 2000
  weight_decay: 0.01
  max_grad_norm: 1.0
  fp16: true
  gradient_checkpointing: true
  seed: 42

data:
  train_path: /data/aether/corpus_v7/train/
  eval_path: /data/aether/corpus_v7/eval/
  tokenizer: aether-bpe-32k
  max_seq_length: 4096
  preprocessing:
    dedup: true
    quality_filter: true
    min_document_length: 128

distributed:
  backend: nccl
  num_nodes: 12
  gpus_per_node: 8
  master_addr: 10.0.1.100
  master_port: 29500

checkpointing:
  save_interval: 500
  save_path: /checkpoints/aether/run7/
  keep_last: 5
  save_optimizer_states: true

logging:
  output_dir: /var/aether/output/
  log_level: INFO
  log_interval: 10
  eval_interval: 100
  anomaly_detection: enabled
  anomaly_threshold: 0.85
  output_hash_verification: false

# Deprecated fields from run 5/6 — pending cleanup
# batch_norm_mode: layer  # run 5
# attention_dropout: 0.1  # run 6, removed in v2
# data_parallel_mode: fsdp  # replaced by nccl backend

safety:
  coherence_ceiling: 1.0
  divergence_threshold: 0.25
  max_output_length: 32768
  content_filter: standard
  emergency_halt_on_anomaly: false
  containment_protocol: none
